{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorboardX import SummaryWriter\n",
    "sess = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "gpu_options = tf.GPUOptions(allow_growth=True,per_process_gpu_memory_fraction=0.8)\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(8,)\n",
      "Box(2,)\n",
      "[-1. -1.]\n",
      "[1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trrrrr/anaconda3/envs/rl/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "from wrappers import make_atari_deepmind\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "env_name = \"LunarLanderContinuous-v2\"\n",
    "#env_name = \"Pendulum-v0\"\n",
    "#env_name = \"BreakoutNoFrameskip-v4\"\n",
    "#env_name = \"SpaceInvadersNoFrameskip-v4\"\n",
    "env = gym.make(env_name)\n",
    "env_eval = gym.make(env_name)\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "print(observation_space)\n",
    "print(action_space)\n",
    "print(action_space.low)\n",
    "print(action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1013 20:50:12.633558 139620787271488 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/ddpgagent.py:106: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1013 20:50:12.637350 139620787271488 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/networks.py:586: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1013 20:50:12.637768 139620787271488 deprecation.py:323] From /home/trrrrr/Documents/github/ml/dqn_atari/networks.py:588: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W1013 20:50:12.640220 139620787271488 deprecation.py:506] From /home/trrrrr/anaconda3/envs/rl/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1013 20:50:12.954479 139620787271488 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/ddpgagent.py:179: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W1013 20:50:12.954910 139620787271488 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/ddpgagent.py:179: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W1013 20:50:12.957261 139620787271488 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/ddpgagent.py:141: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
      "\n",
      "W1013 20:50:12.966676 139620787271488 deprecation.py:323] From /home/trrrrr/anaconda3/envs/rl/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1013 20:50:12.972430 139620787271488 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/ddpgagent.py:143: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W1013 20:50:13.128388 139620787271488 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/ddpgagent.py:122: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "W1013 20:50:13.160737 139620787271488 deprecation_wrapper.py:119] From /home/trrrrr/Documents/github/ml/dqn_atari/ddpgagent.py:123: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from ddpgagent import DDPGAgent\n",
    "import tr_helpers\n",
    "import networks\n",
    "import models\n",
    "\n",
    "#agent.epsilon = 0.5\n",
    "\n",
    "dddqn_config = {\n",
    "    'GAMMA' : 0.99,\n",
    "    'ACTOR_LEARNING_RATE' : 1e-4,\n",
    "    'CRITIC_LEARNING_RATE' : 1e-3,\n",
    "    'STEPS_PER_EPOCH' : 1,\n",
    "    'BATCH_SIZE' : 32,\n",
    "    'EPSILON' : 1,\n",
    "    'MIN_EPSILON' : 0.001,\n",
    "    'EPSILON_DECAY_FRAMES' : 200000,\n",
    "    'NAME' : 'DDPG',\n",
    "    'SCORE_TO_WIN' : 200,\n",
    "    'NUM_STEPS_FILL_BUFFER' : 10000,\n",
    "    'REPLAY_BUFFER_TYPE' : 'normal',\n",
    "    'REPLAY_BUFFER_SIZE' : 100000,\n",
    "    'STEPS_NUM' : 1,\n",
    "    'NETWORK' : models.ModelDDPG(networks.default_ddpg_network),\n",
    "    'REWARD_SHAPER' : tr_helpers.DefaultRewardsShaper(scale_value = 1.0/10.0),\n",
    "    'EPISODES_TO_LOG' : 10,\n",
    "    'FRAMES_TO_LOG' : 5*1024, \n",
    "    'LIVES_REWARD' : 1,\n",
    "    'ATOMS_NUM' : 1\n",
    "    }\n",
    "\n",
    "\n",
    "agent = DDPGAgent(env, env_eval, sess, env_name, observation_space, action_space, config = dddqn_config)\n",
    "#agent.restore('nn/pong_dddqn_config0PongNoFrameskip-v4')\n",
    "#agent.epsilon = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per seconds:  267.80798189841204\n",
      "Frames per seconds:  287.93813580258086\n",
      "Frames per seconds:  283.71368625426777\n",
      "Frames per seconds:  285.0558322767825\n",
      "Frames per seconds:  273.3287930497718\n",
      "mean_rewards:  -383.7793413313276 mean_steps:  195.8\n",
      "saving next best rewards:  -383.7793413313276\n",
      "Frames per seconds:  279.9167852873314\n",
      "Frames per seconds:  265.5680677781862\n",
      "Frames per seconds:  259.4807744729389\n",
      "Frames per seconds:  259.05680625220117\n",
      "Frames per seconds:  258.7601862219966\n",
      "mean_rewards:  -398.63851842236454 mean_steps:  302.4\n",
      "Frames per seconds:  264.33356084451\n",
      "Frames per seconds:  257.0266071541313\n",
      "Frames per seconds:  264.8411804718007\n",
      "Frames per seconds:  270.0848982341281\n",
      "Frames per seconds:  246.982864031649\n",
      "mean_rewards:  -216.17119487230266 mean_steps:  260.7\n",
      "saving next best rewards:  -216.17119487230266\n",
      "Frames per seconds:  263.3327583908756\n",
      "Frames per seconds:  247.29754387621554\n",
      "Frames per seconds:  242.09706559775628\n",
      "Frames per seconds:  267.2674305118058\n",
      "Frames per seconds:  250.07077411962626\n",
      "mean_rewards:  -271.6202946029149 mean_steps:  133.1\n",
      "Frames per seconds:  265.60173449456903\n",
      "Frames per seconds:  265.5328336167251\n",
      "Frames per seconds:  269.71188434967763\n",
      "Frames per seconds:  273.4374124054565\n",
      "Frames per seconds:  268.74200664770797\n",
      "mean_rewards:  -201.0395900658548 mean_steps:  123.5\n",
      "saving next best rewards:  -201.0395900658548\n",
      "Frames per seconds:  260.5964066303269\n",
      "Frames per seconds:  261.0644957605721\n",
      "Frames per seconds:  276.81235137738236\n",
      "Frames per seconds:  241.0851584376575\n",
      "Frames per seconds:  263.99161907815267\n",
      "mean_rewards:  60.83241728424836 mean_steps:  286.6\n",
      "saving next best rewards:  60.83241728424836\n",
      "Frames per seconds:  238.1761972715059\n",
      "Frames per seconds:  240.70399398498836\n",
      "Frames per seconds:  175.6315052033121\n",
      "Frames per seconds:  231.85263194148177\n",
      "Frames per seconds:  264.8224991948903\n",
      "mean_rewards:  -129.67242618109665 mean_steps:  162.4\n",
      "Frames per seconds:  270.2827371524103\n",
      "Frames per seconds:  268.2259212631333\n",
      "Frames per seconds:  263.3528125558748\n",
      "Frames per seconds:  255.61550754616053\n",
      "Frames per seconds:  240.96840164862914\n",
      "mean_rewards:  -149.60158937591143 mean_steps:  392.1\n",
      "Frames per seconds:  272.5637404838357\n",
      "Frames per seconds:  269.65706994110064\n",
      "Frames per seconds:  233.58014500352493\n",
      "Frames per seconds:  202.49527449280322\n",
      "Frames per seconds:  271.1827234264321\n",
      "mean_rewards:  -114.58119563767193 mean_steps:  73.6\n",
      "Frames per seconds:  274.50477370312444\n",
      "Frames per seconds:  286.3796084964243\n",
      "Frames per seconds:  291.26720005251667\n",
      "Frames per seconds:  309.10900403600834\n",
      "Frames per seconds:  318.303061654105\n",
      "mean_rewards:  -104.2741387915825 mean_steps:  121.3\n",
      "Frames per seconds:  308.53909798428685\n",
      "Frames per seconds:  308.8958730334079\n",
      "Frames per seconds:  277.8538506082005\n",
      "Frames per seconds:  279.1777807894974\n",
      "Frames per seconds:  309.4073292418735\n",
      "mean_rewards:  -122.45957753870584 mean_steps:  70.7\n",
      "Frames per seconds:  306.46684181909256\n",
      "Frames per seconds:  274.6497147777763\n",
      "Frames per seconds:  280.02239778974257\n",
      "Frames per seconds:  284.3622478202425\n",
      "Frames per seconds:  285.6832425181849\n",
      "mean_rewards:  -127.34347848815585 mean_steps:  103.9\n",
      "Frames per seconds:  282.04286059700553\n",
      "Frames per seconds:  281.3301163857471\n",
      "Frames per seconds:  280.8663183854203\n",
      "Frames per seconds:  280.5458121409133\n",
      "Frames per seconds:  284.50827064222585\n",
      "mean_rewards:  -181.29582362416426 mean_steps:  80.6\n",
      "Frames per seconds:  282.07620279325215\n",
      "Frames per seconds:  284.32889006755164\n",
      "Frames per seconds:  283.2714437785994\n",
      "Frames per seconds:  285.21586396660257\n",
      "Frames per seconds:  285.18753205064144\n",
      "mean_rewards:  -121.18385943751355 mean_steps:  69.6\n",
      "Frames per seconds:  274.4125559363427\n",
      "Frames per seconds:  301.00722170531594\n",
      "Frames per seconds:  317.906565697687\n",
      "Frames per seconds:  318.33840289555997\n",
      "Frames per seconds:  317.19292507425104\n",
      "mean_rewards:  -137.40560628518008 mean_steps:  67.9\n",
      "Frames per seconds:  308.5213451345742\n",
      "Frames per seconds:  308.31723450471566\n",
      "Frames per seconds:  279.06498911053484\n",
      "Frames per seconds:  293.72535601562316\n",
      "Frames per seconds:  302.2285564743514\n",
      "mean_rewards:  -439.4402928072794 mean_steps:  70.7\n",
      "Frames per seconds:  289.3521242750899\n",
      "Frames per seconds:  300.83119669179825\n",
      "Frames per seconds:  308.3258443921096\n",
      "Frames per seconds:  308.46143050059686\n",
      "Frames per seconds:  307.3972065933946\n",
      "mean_rewards:  -462.3023335713635 mean_steps:  71.3\n",
      "Frames per seconds:  277.32702852383477\n",
      "Frames per seconds:  278.6884384880294\n",
      "Frames per seconds:  282.2328684001526\n",
      "Frames per seconds:  277.75074736214\n",
      "Frames per seconds:  279.24539426889163\n",
      "mean_rewards:  -515.0746083216567 mean_steps:  71.3\n",
      "Frames per seconds:  276.6211245858219\n",
      "Frames per seconds:  308.4993619150292\n",
      "Frames per seconds:  318.01825919996065\n",
      "Frames per seconds:  285.49345816172854\n",
      "Frames per seconds:  278.9178875072409\n",
      "mean_rewards:  -734.0682766885141 mean_steps:  79.3\n",
      "Frames per seconds:  280.8751532154564\n",
      "Frames per seconds:  313.79027019191835\n",
      "Frames per seconds:  305.03467983081845\n",
      "Frames per seconds:  314.3169723821954\n",
      "Frames per seconds:  319.65585090393336\n",
      "mean_rewards:  -536.3168054291084 mean_steps:  73.2\n",
      "Frames per seconds:  318.5596171618532\n",
      "Frames per seconds:  306.4428764723997\n",
      "Frames per seconds:  311.0295312296666\n",
      "Frames per seconds:  316.578637921648\n",
      "Frames per seconds:  310.19149657161756\n",
      "mean_rewards:  -490.62698420239883 mean_steps:  73.7\n",
      "Frames per seconds:  311.20885715122097\n",
      "Frames per seconds:  296.51444757798663\n",
      "Frames per seconds:  293.73827276066635\n",
      "Frames per seconds:  275.48591537867736\n",
      "Frames per seconds:  275.81165959483184\n",
      "mean_rewards:  -412.1237895791234 mean_steps:  75.8\n",
      "Frames per seconds:  276.36545172614865\n",
      "Frames per seconds:  276.3670699983598\n",
      "Frames per seconds:  280.415745526029\n",
      "Frames per seconds:  279.8283161048123\n",
      "Frames per seconds:  278.6594177900771\n",
      "mean_rewards:  -399.57545733421296 mean_steps:  78.4\n",
      "Frames per seconds:  274.64107405876047\n",
      "Frames per seconds:  277.01215038075435\n",
      "Frames per seconds:  275.78157034368166\n",
      "Frames per seconds:  276.7859675470332\n",
      "Frames per seconds:  275.16532923766323\n",
      "mean_rewards:  -325.4146165567876 mean_steps:  78.8\n",
      "Frames per seconds:  277.5667784869697\n",
      "Frames per seconds:  281.1886623344297\n",
      "Frames per seconds:  280.2820498112307\n",
      "Frames per seconds:  279.73044695338984\n",
      "Frames per seconds:  281.6008593776226\n",
      "mean_rewards:  -389.6952165943458 mean_steps:  63.9\n",
      "Frames per seconds:  279.87641918625786\n",
      "Frames per seconds:  293.1854376858448\n",
      "Frames per seconds:  307.8948125816743\n",
      "Frames per seconds:  280.2594077339526\n",
      "Frames per seconds:  279.472924756206\n",
      "mean_rewards:  -381.6777809998985 mean_steps:  77.7\n",
      "Frames per seconds:  279.0852623990014\n",
      "Frames per seconds:  277.3264554987733\n",
      "Frames per seconds:  274.07193374185573\n",
      "Frames per seconds:  275.22166540941635\n",
      "Frames per seconds:  276.8838752433631\n",
      "mean_rewards:  -230.86304723196244 mean_steps:  98.2\n",
      "Frames per seconds:  272.9860134122005\n",
      "Frames per seconds:  276.72868610732166\n",
      "Frames per seconds:  278.00418634124935\n",
      "Frames per seconds:  284.1687426029552\n",
      "Frames per seconds:  276.392893784845\n",
      "mean_rewards:  -327.3434807695164 mean_steps:  88.8\n",
      "Frames per seconds:  277.0122039800539\n",
      "Frames per seconds:  277.4081892037724\n",
      "Frames per seconds:  276.9948746206528\n",
      "Frames per seconds:  279.77714951749965\n",
      "Frames per seconds:  282.70335286754874\n",
      "mean_rewards:  -126.59569469863553 mean_steps:  66.6\n",
      "Frames per seconds:  283.00840795060026\n",
      "Frames per seconds:  279.68920568572435\n",
      "Frames per seconds:  281.51314935320306\n",
      "Frames per seconds:  282.7645681645227\n",
      "Frames per seconds:  282.5377819700842\n",
      "mean_rewards:  -296.9613057249565 mean_steps:  68.8\n",
      "Frames per seconds:  280.12469172189685\n",
      "Frames per seconds:  283.9951978988824\n",
      "Frames per seconds:  284.4738234732101\n",
      "Frames per seconds:  280.4169172537893\n",
      "Frames per seconds:  283.87232667231814\n",
      "mean_rewards:  -178.2157243947652 mean_steps:  74.6\n",
      "Frames per seconds:  278.1864833890639\n",
      "Frames per seconds:  282.106180451594\n",
      "Frames per seconds:  281.31830468635326\n",
      "Frames per seconds:  283.5612137066942\n",
      "Frames per seconds:  284.3843714137169\n",
      "mean_rewards:  -190.0085498711349 mean_steps:  80.5\n",
      "Frames per seconds:  280.2577252700364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per seconds:  282.4691779716033\n",
      "Frames per seconds:  276.87466500811354\n",
      "Frames per seconds:  278.37295285758654\n",
      "Frames per seconds:  277.65913632546784\n",
      "mean_rewards:  -104.01173753970193 mean_steps:  87.5\n",
      "Frames per seconds:  277.513692028919\n",
      "Frames per seconds:  275.9347059631518\n",
      "Frames per seconds:  275.3460039618072\n",
      "Frames per seconds:  281.10269908423965\n",
      "Frames per seconds:  278.09975264797333\n",
      "mean_rewards:  -173.8081075881218 mean_steps:  79.2\n",
      "Frames per seconds:  279.8647839453481\n",
      "Frames per seconds:  283.3731155709269\n",
      "Frames per seconds:  280.73487183750905\n",
      "Frames per seconds:  281.82379481024253\n",
      "Frames per seconds:  280.9903875040693\n",
      "mean_rewards:  -385.1997048094497 mean_steps:  103.9\n",
      "Frames per seconds:  275.4611794638439\n",
      "Frames per seconds:  276.7035839088586\n",
      "Frames per seconds:  275.89200650300717\n",
      "Frames per seconds:  266.2309637832632\n",
      "Frames per seconds:  292.085569822482\n",
      "mean_rewards:  -227.4178400856844 mean_steps:  119.4\n",
      "Frames per seconds:  304.6992355147931\n",
      "Frames per seconds:  306.58252293568813\n",
      "Frames per seconds:  308.8189140406254\n",
      "Frames per seconds:  276.02326659282\n",
      "Frames per seconds:  294.7585556295298\n",
      "mean_rewards:  -530.7643488063728 mean_steps:  132.8\n",
      "Frames per seconds:  266.4362350579778\n",
      "Frames per seconds:  259.68950307082906\n",
      "Frames per seconds:  263.03208648438374\n",
      "Frames per seconds:  299.85838449873245\n",
      "Frames per seconds:  294.19887433653196\n",
      "mean_rewards:  -282.9815494791786 mean_steps:  128.1\n",
      "Frames per seconds:  284.96081450448986\n",
      "Frames per seconds:  313.8527546580086\n",
      "Frames per seconds:  301.88161862916866\n",
      "Frames per seconds:  297.6187200066301\n",
      "Frames per seconds:  304.1287901409688\n",
      "mean_rewards:  -509.2661567086226 mean_steps:  163.9\n",
      "Frames per seconds:  311.63864154121757\n",
      "Frames per seconds:  314.83838339411705\n",
      "Frames per seconds:  289.768620255028\n",
      "Frames per seconds:  267.5766853519059\n",
      "Frames per seconds:  280.22431791446274\n",
      "mean_rewards:  -616.9102737463614 mean_steps:  231.1\n",
      "Frames per seconds:  282.61117978859994\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.save(\"./nn/ENDDDQN2\" + env_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddpgagent import rescale_actions\n",
    "\n",
    "import wrappers\n",
    "#print(env.unwrapped.get_action_meanings())\n",
    "def evaluate(env,t_max=10000):\n",
    "    rewards = []\n",
    "    #env._max_episode_steps = 200\n",
    "    print('reset')\n",
    "    #env = env.old_env\n",
    "    s = env.reset()\n",
    "    reward = 0\n",
    "    for it in range(t_max):\n",
    "        env.render()\n",
    "        action = agent.get_action(s)\n",
    "        action = rescale_actions(agent.actions_low, agent.actions_high, action)\n",
    "        #print(action)\n",
    "        s, r, done, _ = env.step(action)\n",
    "        reward += r\n",
    "        \n",
    "            \n",
    "        if done:\n",
    "            break       \n",
    "        \n",
    "    return reward\n",
    "\n",
    "import gym.wrappers\n",
    "\n",
    "\n",
    "#env_monitor = gym.wrappers.Monitor(env,directory='video_dddqn05',force=True)\n",
    "\n",
    "sessions = [print('reward:', evaluate(env)) for _ in range(10)]\n",
    "env_monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#agent = DQNAgent(env, sess, ExperienceBuffer(EXP_BUFFER_CAPACITY), env_name, config = dqn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.reset()\n",
    "#agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
